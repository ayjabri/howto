{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import models, transforms\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a 'Dataset' class\n",
    "\n",
    "You must creae a 'Dataset' class for each dataset and define:\n",
    "1. `__init__`\n",
    "2. `__getitem__`\n",
    "3. `__len__`\n",
    "\n",
    "Define a set of parameters for a random dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters and DataLoaders\n",
    "input_size = 5\n",
    "output_size = 2\n",
    "\n",
    "batch_size = 30\n",
    "data_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length, size)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 5])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=RandomDataset(input_size, data_size);dataset.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_loader = DataLoader(dataset,\n",
    "                         batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(rand_loader))\n",
    "x=batch #there is only one value in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 5])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([30, 5])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom DataSet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom dataset from a cvs file that contains independent and dependent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class California_Dataset(Dataset):\n",
    "    \"\"\"California House Prices Dataset.\"\"\"\n",
    "\n",
    "    def __init__(self,csv_file,normalize=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.raw_data = pd.read_csv(csv_file)\n",
    "        self.normalize = normalize\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.raw_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        data = self.raw_data.iloc[:,:8].values\n",
    "        data = torch.from_numpy(data).float()\n",
    "        targets = self.raw_data.iloc[:,-1:].values\n",
    "        targets = torch.from_numpy(targets).float().view(-1,1)\n",
    "        if self.normalize: \n",
    "            data = F.normalize(data,dim=0)\n",
    "            targets = F.normalize(targets,dim=0)\n",
    "        sample = (data[idx],targets[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = California_Dataset('~/notebooks/California/california_housing_train.csv',normalize=False)\n",
    "\n",
    "trainloader = DataLoader(trainset,batch_size=100,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(trainloader))\n",
    "x,y=batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-122.1500,   37.7200,   47.0000, 1190.0000,  251.0000,  540.0000,\n",
       "          266.0000,    3.3750]), tensor([198300.]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0],y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset from X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.randn(1000,3,28,28)\n",
    "y = torch.randint(0,10,(1000,))\n",
    "classes = {'A':0,'B':1,'C':2,'D':3,'E':4,'F':5,'G':6,'H':7,'I':8,'J':9}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,x,y,classes,transform=None):\n",
    "        self.data = x\n",
    "        self.targets = y\n",
    "        self.classes_to_idx = classes\n",
    "        self.classes = list(classes.keys())\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self,idx):\n",
    "        img,target = self.data[idx],self.targets[idx]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img,target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aboce is a simple normally distributed dataset.\n",
    "\n",
    "To iterate through the data, all we need to do is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the dataloader\n",
    "xy_dataset = MyDataSet(X,y,classes,transform=tfms)\n",
    "xy_loader = DataLoader(xy_dataset,batch_size=100,shuffle=True,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'A': 0,\n",
       " 'B': 1,\n",
       " 'C': 2,\n",
       " 'D': 3,\n",
       " 'E': 4,\n",
       " 'F': 5,\n",
       " 'G': 6,\n",
       " 'H': 7,\n",
       " 'I': 8,\n",
       " 'J': 9}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_loader.dataset.classes_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([100, 3, 28, 28]), torch.Size([100]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img,label = next(iter(xy_loader))\n",
    "img.shape,label.shape,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset with limited number of datapoints\n",
    "\n",
    "To create a dataset with n number of datapoints from a list of big data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random data of 100 points\n",
    "x = torch.randn(1000,2,32,32)\n",
    "y = torch.randint(0,10,(1000,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LimiteDataset(Dataset):\n",
    "    def __init__(self,x,y,n=None,transform=None):\n",
    "        self.data = x\n",
    "        self.targets = y\n",
    "        self.classes = torch.unique(y)\n",
    "        self.transform = transform\n",
    "        self.max_size = n #define size\n",
    "    def __len__(self):\n",
    "        return self.max_size if self.max_size is not None else len(self.data) #this line to control size\n",
    "    def __getitem__(self,idx):\n",
    "        img,targets = self.data[idx],self.targets[idx]\n",
    "        if transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img,targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_dataset = LimiteDataset(x,y,n=300,transform=tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "limit_loader = DataLoader(limit_dataset,batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(limit_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lin1 = nn.Linear(8,100)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.lin2 = nn.Linear(100,100)\n",
    "        self.bn2 = nn.BatchNorm1d(100)\n",
    "        self.lin3 = nn.Linear(100,100)\n",
    "        self.bn3 = nn.BatchNorm1d(100)\n",
    "        self.lin4 = nn.Linear(100,1)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.bn1(self.lin1(x)))\n",
    "        x = F.relu(self.bn2(self.lin2(x)))\n",
    "        x = F.relu(self.bn3(self.lin3(x)))\n",
    "        x = self.lin4(x)\n",
    "        return x\n",
    "    \n",
    "    def learn(self,epochs,dataloader,lr):\n",
    "        loss_fn = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(self.parameters(),lr=lr)\n",
    "        for epoch in range(epochs):\n",
    "            losses = []\n",
    "            correct=0\n",
    "            total=0\n",
    "            for b,data in enumerate(dataloader):\n",
    "                optimizer.zero_grad()\n",
    "                x,y = data\n",
    "                output = self(x)\n",
    "                loss = loss_fn(output,y)\n",
    "                loss.backward()\n",
    "                losses.append(loss.item())\n",
    "                with torch.no_grad():\n",
    "                    predict = self(x)\n",
    "                    for idx,i in enumerate(predict):\n",
    "                        if i==y[idx]: correct+=1\n",
    "                        total+=1\n",
    "                optimizer.step()\n",
    "            #if epoch%(epochs/10)==0:\n",
    "            print('Epoch: {} Loss: {}'.format(epoch,round(loss.item(),8)))\n",
    "            print('Average Loss: {} Accuracy {}'.format(loss.mean(),correct/total*100))\n",
    "        \n",
    "                    \n",
    "            \n",
    "net = Net()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(net,epochs,train,val,lr):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(),lr=lr)\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        correct=0\n",
    "        total=0\n",
    "        eloss=[]\n",
    "        for b,data in enumerate(train):\n",
    "            bloss=[]\n",
    "            optimizer.zero_grad()\n",
    "            x,y = data\n",
    "            output = net(x)\n",
    "            loss = loss_fn(output,y)\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            eloss.append(loss.item())\n",
    "            bloss.append(loss.item())\n",
    "            for idx,i in enumerate(output):\n",
    "                if i==y[idx]: correct+=1\n",
    "                total+=1\n",
    "            optimizer.step()\n",
    "            #if b%(len(train)/10)==0:print('Avg Loss {}'.format(np.mean(bloss)))\n",
    "        accuracy=round(correct/total*100,5)\n",
    "        print('Epoch:{} Avg.Training Loss: {} Accuracy {}%'.format(epoch,round(np.mean(eloss),8),accuracy))\n",
    "    return losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0 Avg.Training Loss: 3655968906.5411763 Accuracy 0.0%\n",
      "Epoch:1 Avg.Training Loss: 3631510902.964706 Accuracy 0.0%\n",
      "Epoch:2 Avg.Training Loss: 3615545394.4470587 Accuracy 0.0%\n",
      "Epoch:3 Avg.Training Loss: 3568090072.847059 Accuracy 0.00588%\n",
      "Epoch:4 Avg.Training Loss: 3611755713.5058823 Accuracy 0.0%\n",
      "Epoch:5 Avg.Training Loss: 3544755456.0 Accuracy 0.0%\n",
      "Epoch:6 Avg.Training Loss: 3583977121.882353 Accuracy 0.0%\n",
      "Epoch:7 Avg.Training Loss: 3607246887.152941 Accuracy 0.0%\n",
      "Epoch:8 Avg.Training Loss: 3550331577.9764705 Accuracy 0.0%\n",
      "Epoch:9 Avg.Training Loss: 3581355556.141177 Accuracy 0.0%\n",
      "Epoch:10 Avg.Training Loss: 3668213839.8117647 Accuracy 0.0%\n",
      "Epoch:11 Avg.Training Loss: 3608610880.0 Accuracy 0.0%\n",
      "Epoch:12 Avg.Training Loss: 3560448795.105882 Accuracy 0.0%\n",
      "Epoch:13 Avg.Training Loss: 3545008928.3764706 Accuracy 0.0%\n",
      "Epoch:14 Avg.Training Loss: 3589204509.364706 Accuracy 0.0%\n",
      "Epoch:15 Avg.Training Loss: 3608514659.388235 Accuracy 0.0%\n",
      "Epoch:16 Avg.Training Loss: 3496246485.082353 Accuracy 0.0%\n",
      "Epoch:17 Avg.Training Loss: 3589586858.917647 Accuracy 0.0%\n",
      "Epoch:18 Avg.Training Loss: 3540932537.9764705 Accuracy 0.0%\n",
      "Epoch:19 Avg.Training Loss: 3631939399.529412 Accuracy 0.0%\n",
      "Epoch:20 Avg.Training Loss: 3577508968.6588235 Accuracy 0.0%\n",
      "Epoch:21 Avg.Training Loss: 3562031511.3411765 Accuracy 0.0%\n",
      "Epoch:22 Avg.Training Loss: 3528670611.5764704 Accuracy 0.0%\n",
      "Epoch:23 Avg.Training Loss: 3490308200.6588235 Accuracy 0.0%\n",
      "Epoch:24 Avg.Training Loss: 3550510930.070588 Accuracy 0.0%\n",
      "Epoch:25 Avg.Training Loss: 3504473490.823529 Accuracy 0.0%\n",
      "Epoch:26 Avg.Training Loss: 3559089728.0 Accuracy 0.0%\n",
      "Epoch:27 Avg.Training Loss: 3486463671.7176476 Accuracy 0.0%\n",
      "Epoch:28 Avg.Training Loss: 3590153606.77647 Accuracy 0.0%\n",
      "Epoch:29 Avg.Training Loss: 3573109701.2705884 Accuracy 0.0%\n",
      "Epoch:30 Avg.Training Loss: 3496935188.329412 Accuracy 0.0%\n",
      "Epoch:31 Avg.Training Loss: 3510535143.9058824 Accuracy 0.0%\n",
      "Epoch:32 Avg.Training Loss: 3560374795.294118 Accuracy 0.0%\n",
      "Epoch:33 Avg.Training Loss: 3552690943.2470584 Accuracy 0.0%\n",
      "Epoch:34 Avg.Training Loss: 3488912320.0 Accuracy 0.0%\n",
      "Epoch:35 Avg.Training Loss: 3586198911.2470584 Accuracy 0.0%\n",
      "Epoch:36 Avg.Training Loss: 3527383599.435294 Accuracy 0.0%\n",
      "Epoch:37 Avg.Training Loss: 3502379769.9764705 Accuracy 0.0%\n",
      "Epoch:38 Avg.Training Loss: 3498236422.77647 Accuracy 0.0%\n",
      "Epoch:39 Avg.Training Loss: 3478475420.611765 Accuracy 0.0%\n",
      "Epoch:40 Avg.Training Loss: 3481116377.6 Accuracy 0.0%\n",
      "Epoch:41 Avg.Training Loss: 3502490698.5411763 Accuracy 0.0%\n",
      "Epoch:42 Avg.Training Loss: 3473354191.0588236 Accuracy 0.0%\n",
      "Epoch:43 Avg.Training Loss: 3479750080.7529416 Accuracy 0.0%\n",
      "Epoch:44 Avg.Training Loss: 3513941950.4941177 Accuracy 0.0%\n",
      "Epoch:45 Avg.Training Loss: 3451504318.4941177 Accuracy 0.0%\n",
      "Epoch:46 Avg.Training Loss: 3429520563.2 Accuracy 0.0%\n",
      "Epoch:47 Avg.Training Loss: 3483973793.1294117 Accuracy 0.0%\n",
      "Epoch:48 Avg.Training Loss: 3439083177.4117646 Accuracy 0.0%\n",
      "Epoch:49 Avg.Training Loss: 3472782486.5882354 Accuracy 0.0%\n"
     ]
    }
   ],
   "source": [
    "losses = learn(net,50,trainloader,trainloader,1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
