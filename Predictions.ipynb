{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fake dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tv.transforms.Compose([tv.transforms.ToTensor(),\n",
    "                             tv.transforms.Normalize((0.5,0.5,0.5),(0.5,.5,.5))])\n",
    "ds = tv.datasets.FakeData(transform=tfms)\n",
    "dl = torch.utils.data.DataLoader(ds,batch_size=100,shuffle=True)\n",
    "\n",
    "net = tv.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False) # disable grad \n",
    "\n",
    "net.fc = nn.Linear(in_features=512, out_features=10, bias=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in net.parameters():\n",
    "    i.requires_grad = False\n",
    "net.fc.weight.requires_grad = True\n",
    "net.fc.bias.requiers_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Batch Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(dl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "one batch prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 3, 1, 1, 3, 1, 1, 3,\n",
       "        1, 1, 1, 1, 1, 1, 3, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1,\n",
       "        1, 1, 1, 1, 3, 1, 1, 1, 3, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1,\n",
       "        1, 1, 3, 1])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict = net(x)\n",
    "predict = F.softmax(predict,dim=1)\n",
    "predict.argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compare with y use `eq` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.argmax(dim=1).eq(y).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a prediction function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_correct(predict,label):\n",
    "    correct = predict.argmax(dim=1).eq(label).sum()\n",
    "    return correct.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_correct(predict,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4256, 0.2751, 0.4044, 0.2591, 0.3352, 0.2602, 0.3783, 0.5520, 0.2984,\n",
       "        0.2858, 0.3104, 0.4339, 0.3315, 0.2444, 0.2064, 0.2362, 0.3658, 0.2935,\n",
       "        0.3740, 0.3841, 0.3118, 0.3776, 0.1776, 0.3917, 0.2164, 0.4122, 0.2490,\n",
       "        0.3091, 0.2659, 0.2879, 0.3794, 0.2401, 0.4450, 0.3219, 0.2243, 0.2090,\n",
       "        0.3554, 0.3440, 0.3170, 0.2257, 0.2632, 0.2089, 0.3746, 0.2181, 0.2153,\n",
       "        0.3278, 0.2771, 0.3504, 0.2965, 0.2322, 0.3615, 0.2273, 0.2188, 0.2409,\n",
       "        0.3122, 0.3790, 0.2078, 0.2924, 0.2624, 0.2218, 0.3648, 0.3025, 0.3176,\n",
       "        0.2622, 0.3791, 0.2606, 0.2921, 0.3260, 0.3552, 0.3873, 0.2705, 0.3419,\n",
       "        0.4435, 0.2816, 0.2805, 0.3282, 0.3385, 0.4122, 0.3644, 0.2839, 0.2487,\n",
       "        0.3154, 0.4441, 0.3942, 0.2153, 0.2430, 0.1983, 0.3332, 0.2490, 0.3064,\n",
       "        0.2298, 0.2281, 0.2056, 0.3268, 0.3742, 0.2350, 0.2080, 0.3700, 0.2086,\n",
       "        0.2535])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.max(dim=1).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Predictions for the entire training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Loop through the dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def get_all_preds(model, loader):\n",
    "    all_preds = torch.tensor([])\n",
    "    targets\n",
    "    for batch in loader:\n",
    "        images, labels = batch\n",
    "\n",
    "        preds = model(images)\n",
    "        all_preds = torch.cat(\n",
    "            (all_preds, preds)\n",
    "            ,dim=0\n",
    "        )\n",
    "    return all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6681,  0.7576, -0.0761,  ...,  0.2774,  0.3135,  0.1969],\n",
       "        [ 0.5639,  1.4598, -0.2292,  ...,  0.1145,  0.0816, -0.1499],\n",
       "        [ 0.6646,  1.8453, -0.3320,  ...,  0.6000,  0.0594,  0.4885],\n",
       "        ...,\n",
       "        [ 0.6180,  1.6585, -0.2911,  ...,  0.3005,  0.3355,  0.4243],\n",
       "        [ 0.8215,  1.7869, -0.0797,  ...,  0.4893,  0.1407,  0.2800],\n",
       "        [ 0.3575,  1.4516, -0.0435,  ...,  0.5190,  0.0772,  0.0443]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_all_preds(net,dl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Create a dataloader where batch size equals sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    dll = torch.utils.data.DataLoader(ds,batch_size=len(ds))\n",
    "    all_pred = get_all_preds(net,dll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 10])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: most datasets have the attribute `.targets`. Which means we don't need to loop through all pathers to get them, but in case it's missing, you can extract it by adding `label = torch.tensor([])` to `get_all_preds`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
